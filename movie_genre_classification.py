# -*- coding: utf-8 -*-
"""movie_genre_classification.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1LGJU5P8ObQjqmngwLCtp679O8oYjRogl

#**Movie Genre Classification**

##1. **Introduction:**
This project builds a machine learning model to predict movie genres from plot summaries. Using text preprocessing techniques like TF-IDF and classifiers such as Naive Bayes, Logistic Regression, or SVM, the model is trained and evaluated on metrics like accuracy and F1-score for robust performance.

###**2. Imporing Libraries and loading datasets**
"""

import pandas as pd
import numpy as np
import nltk
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.naive_bayes import MultinomialNB
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
import gensim
import matplotlib.pyplot as plt
import seaborn as sns

df=pd.read_csv('/content/train_data.txt', sep=':::', names=['Title','Genre','Description'])

#reviewing the dataset

df.head()

df.shape

#Checking for null values

df.isnull().sum()

"""##**3. Data Wrangling:**
Now we will clean, structure, and transform raw data into a format suitable for analysis

###3.1. Looking for different Genres
"""

classes = df['Genre'].unique()
classes

#Count of Genres in the dataset
n_classes = df['Genre'].nunique()
n_classes

"""###3.2. Handling Duplicates"""

# Check for duplicates and remove them
print(f"Duplicates before: {df.duplicated().sum()}")
df = df.drop_duplicates()
print(f"Duplicates after: {df.duplicated().sum()}")

"""###**4. EDA (Exploatory Data Analysis)**
We will now visualize the Genre with the help of a bar diagram
"""

plt.figure(figsize=(20,10))
sns.countplot(data=df,x='Genre', hue='Genre', legend=False)
plt.title('Distribution of Genres', fontsize=20, weight='bold')
plt.xticks(rotation=45)
plt.show()

"""Here we can see that the genre which we have the most is **drama**, followed by **documentary**, and **comedy**.

##**5. Text Preprocessing:**
We will preprocess the text data (plot summaries) to clean it and prepare it for vectorization. The process includes:
1. Lowercasing text.
2. Removing punctuation, special characters, and stopwords.
3. Tokenizing text into words.
"""

import re
import nltk
from nltk.corpus import stopwords

# Download stopwords if not already done
nltk.download('stopwords')
stop_words = set(stopwords.words('english'))

# Function to clean text
def clean_text(text):
    text = text.lower()  # Convert to lowercase
    text = re.sub(r'[^\w\s]', '', text)  # Remove punctuation
    text = re.sub(r'\d+', '', text)  # Remove numbers
    text = ' '.join(word for word in text.split() if word not in stop_words)  # Remove stopwords
    return text

# Apply cleaning function to the Description column
df['Cleaned_Plot'] = df['Description'].apply(clean_text)

# Display a few examples
print(df[['Description', 'Cleaned_Plot']].head())

"""##**6. Vectorize the Text Using TF-IDF:**
Weâ€™ll transform the cleaned text data into numerical features using TF-IDF (Term Frequency-Inverse Document Frequency). This will convert the plot summaries into vectors that can be used as input for machine learning models.
"""

from sklearn.feature_extraction.text import TfidfVectorizer

# Initialize TF-IDF Vectorizer
tfidf_vectorizer = TfidfVectorizer(max_features=5000)  # Limit to 5000 features to keep it manageable

# Fit and transform the cleaned text
X = tfidf_vectorizer.fit_transform(df['Cleaned_Plot'])

# Convert to array for easier manipulation
X_array = X.toarray()

# Print shape of resulting matrix
print(f"TF-IDF matrix shape: {X_array.shape}")

"""* The TF-IDF vectorizer converts each plot summary into a numerical vector of size 5000 (or the number of features you choose).
* The resulting X is a sparse matrix, with each row representing a movie and each column representing a word feature.

##**7. Encode the Genres:**
We need to convert the genres (categorical labels) into numerical format since machine learning models work with numbers. We will use **LabelEncoder** to transform the genres into integers.
"""

from sklearn.preprocessing import LabelEncoder

# Initialize the LabelEncoder
label_encoder = LabelEncoder()

# Fit and transform the target variable (Genre)
y = label_encoder.fit_transform(df['Genre'])

# Display the mapping of genres to numerical labels
label_mapping = dict(zip(label_encoder.classes_, range(len(label_encoder.classes_))))
print("Label Mapping:", label_mapping)

# Check the transformed target variable
print("Encoded Genres:", y[:10])  # Show the first 10 encoded genres

"""* **labelencoder** assigns a unique number to each genre.
* The y variable now contains the numerical labels for the genres, ready for training the model.

##**8. Splitting the data:**
"""

from sklearn.model_selection import train_test_split

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Print the shapes of the resulting datasets
print(f"Training set shape: {X_train.shape}, {y_train.shape}")
print(f"Testing set shape: {X_test.shape}, {y_test.shape}")

"""##**9. Logistic Regression:**"""

from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report

# Initialize Logistic Regression model
logreg = LogisticRegression(max_iter=1000, random_state=42)

# Train the model on the training data
logreg.fit(X_train, y_train)

# Make predictions on the test data
y_pred = logreg.predict(X_test)

# Evaluate the model
print("Model Performance Metrics:")
print(f"Accuracy: {accuracy_score(y_test, y_pred):.2f}")
print(f"Precision: {precision_score(y_test, y_pred, average='weighted', zero_division=1):.2f}")
print(f"Recall: {recall_score(y_test, y_pred, average='weighted', zero_division=1):.2f}")
print(f"F1-Score: {f1_score(y_test, y_pred, average='weighted', zero_division=1):.2f}")
print("\nClassification Report:")
print(classification_report(y_test, y_pred, zero_division=1))

"""##**10. Naive Bayes Classifier:**
Since we are working with text data, Multinomial Naive Bayes is a good choice as it performs well on word-count.
"""

from sklearn.naive_bayes import MultinomialNB

# Initialize Multinomial Naive Bayes
nb_classifier = MultinomialNB()

# Train the Naive Bayes model on the training data
nb_classifier.fit(X_train, y_train)

# Make predictions on the test data
y_pred_nb = nb_classifier.predict(X_test)

# Evaluate the model
print("Naive Bayes Model Performance Metrics:")
print(f"Accuracy: {accuracy_score(y_test, y_pred_nb):.2f}")
print(f"Precision: {precision_score(y_test, y_pred_nb, average='weighted', zero_division=1):.2f}")
print(f"Recall: {recall_score(y_test, y_pred_nb, average='weighted', zero_division=1):.2f}")
print(f"F1-Score: {f1_score(y_test, y_pred_nb, average='weighted', zero_division=1):.2f}")
print("\nClassification Report:")
print(classification_report(y_test, y_pred_nb, zero_division=1))

"""##**11. Support Vector Machine (SVM):**"""

from sklearn.svm import SVC

# Initialize Support Vector Classifier
svm_classifier = SVC(kernel='linear', random_state=42)

# Train the SVM model on the training data
svm_classifier.fit(X_train, y_train)

# Make predictions on the test data
y_pred_svm = svm_classifier.predict(X_test)

# Evaluate the model
print("SVM Model Performance Metrics:")
print(f"Accuracy: {accuracy_score(y_test, y_pred_svm):.2f}")
print(f"Precision: {precision_score(y_test, y_pred_svm, average='weighted', zero_division=1):.2f}")
print(f"Recall: {recall_score(y_test, y_pred_svm, average='weighted', zero_division=1):.2f}")
print(f"F1-Score: {f1_score(y_test, y_pred_svm, average='weighted', zero_division=1):.2f}")
print("\nClassification Report:")
print(classification_report(y_test, y_pred_svm, zero_division=1))

"""#**12. Conclusion:**
In this assignment, I built a machine learning model to predict movie genres based on plot summaries. The process involved the following key steps:

* I used **TF-IDF** to transform the plot summaries into numerical features that can be used for classification.
* I trained multiple classifiers, including **Logistic Regression**, **Naive Bayes**, and **Support Vector Machine (SVM)**, to predict movie genres.
* The models were evaluated using metrics such as **accuracy**, **precision**, **recall**, and **F1-score**. The **Logistic Regression and SVM models** achieved the **highest accuracy**, both around ***58%***.
* The results highlight that while the models performed reasonably well, there is **still room for improvement**, particularly in **handling class imbalance** and through **hyperparameter tuning**.
"""